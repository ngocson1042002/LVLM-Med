model:
  arch: medlvlm
  model_type: pretrain
  max_txt_len: 1024
  image_size: 448
  end_sym: </s>
  language_model: "meta-llama/Llama-2-7b-chat-hf"
  ckpt: "path/to/checkpoint"
  use_grad_checkpoint: true
  chat_template: true
  low_resource: true
  lora_r: 64
  lora_alpha: 16
  bits: 8
datasets:
  vindrcxr_train:
    vis_processor:
      train:
        name: "blip2_image_eval"
        image_size: 448
    text_processor:
      train:
        name: "blip_caption"
evaluation_datasets:
  vindrcxr_val:
    eval_file_path: "path/to/grounded_diseases_test.json"
    img_path: "path/to/images/test"
    prompt_test: "[radiology] please describe this image in details with radiological features. Use two sentences unless there are no findings. The first sentence should list the global diseases present in the image, and the second should list local diseases with localized bounding boxes. Let's think step by step."
    batch_size: 4
    max_new_tokens: 512
    temperature: 0.6
    top_p: 0.9
    do_sample: false
run:
  task: image_text_pretrain
  name: medlvlm_evaluation
  save_path: "/path/to/save/folder_path"
